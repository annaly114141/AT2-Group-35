{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TWImqalJbCXo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "78peJW8hbHiD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "google_dir = '/content/gdrive/MyDrive/ANLP'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIXXQtEbbOIB",
        "outputId": "fd00f95f-ad96-4cf1-a60e-d9f689751cf6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy data file to Colab instance to avoid quota issues with Google Drive\n",
        "!cp \"/content/gdrive/MyDrive/ANLP/Reviews.csv\" \"/content/\""
      ],
      "metadata": {
        "id": "7u6WD4gabPUB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and check data\n",
        "data = pd.read_csv(\"/content/Reviews.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "h6PeM2VPbXS7",
        "outputId": "c64a3c4f-66b5-4167-a709-dcc0d0ab41eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Id  ...                                               Text\n",
              "0   1  ...  I have bought several of the Vitality canned d...\n",
              "1   2  ...  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2   3  ...  This is a confection that has been around a fe...\n",
              "3   4  ...  If you are looking for the secret ingredient i...\n",
              "4   5  ...  Great taffy at a great price.  There was a wid...\n",
              "\n",
              "[5 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8450e65a-1792-4719-95a6-9ea559a7d788\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8450e65a-1792-4719-95a6-9ea559a7d788')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8450e65a-1792-4719-95a6-9ea559a7d788 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8450e65a-1792-4719-95a6-9ea559a7d788');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out reviews with helpfulness votes\n",
        "helpful_df = data.loc[data['HelpfulnessDenominator'] >= 10].copy()\n",
        "\n",
        "# Create Target Variable\n",
        "helpful_df['helpful_score'] = helpful_df['HelpfulnessNumerator'] / helpful_df['HelpfulnessDenominator']"
      ],
      "metadata": {
        "id": "tU06KrwUbdtZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to label for classification\n",
        "helpful_df['helpful_score'] = np.where(helpful_df['helpful_score'] >= 0.8, 1, 0)\n",
        "helpful_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rQbSlmcibgXu",
        "outputId": "2b3c86fc-f7d0-4d15-ee7a-7618e0f468a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Id  ... helpful_score\n",
              "32    33  ...             1\n",
              "33    34  ...             1\n",
              "82    83  ...             1\n",
              "158  159  ...             1\n",
              "213  214  ...             0\n",
              "\n",
              "[5 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2237137f-2e49-440b-8e33-57d2738495b0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>helpful_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>B001EO5QW8</td>\n",
              "      <td>AOVROBZ8BNTP7</td>\n",
              "      <td>S. Potter</td>\n",
              "      <td>19</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>1163376000</td>\n",
              "      <td>Best of the Instant Oatmeals</td>\n",
              "      <td>McCann's Instant Oatmeal is great if you must ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>B001EO5QW8</td>\n",
              "      <td>A3PMM0NFVEJGK9</td>\n",
              "      <td>Megan \"Bad at Nicknames\"</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>1166313600</td>\n",
              "      <td>Good Instant</td>\n",
              "      <td>This is a good instant oatmeal from the best o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>83</td>\n",
              "      <td>B003ZFRKGO</td>\n",
              "      <td>A2VOZX7YBT0D6D</td>\n",
              "      <td>Johnnycakes \"Johnnycakes\"</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>1325635200</td>\n",
              "      <td>Forget Molecular Gastronomy - this stuff rocke...</td>\n",
              "      <td>I know the product title says Molecular Gastro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>159</td>\n",
              "      <td>B000ITVLE2</td>\n",
              "      <td>A3NID9D9WMIV01</td>\n",
              "      <td>Louie Arrighi \"Lou da Joo\"</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>1260057600</td>\n",
              "      <td>tastes very fresh</td>\n",
              "      <td>&lt;span class=\"tiny\"&gt; Length:: 0:26 Mins&lt;br /&gt;&lt;b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>214</td>\n",
              "      <td>B0009XLVGA</td>\n",
              "      <td>A1NHQNQ3TVXTZF</td>\n",
              "      <td>Desert Girl \"chrissylovesherhusband\"</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1220227200</td>\n",
              "      <td>CHANGED FORMULA MAKES CATS SICK!!!!</td>\n",
              "      <td>As with canidae, Felidae has also changed thei...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2237137f-2e49-440b-8e33-57d2738495b0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2237137f-2e49-440b-8e33-57d2738495b0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2237137f-2e49-440b-8e33-57d2738495b0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy.cli"
      ],
      "metadata": {
        "id": "IaTWrZvDbsDI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the large model over the standard version to get vectors. Requires additional install.\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zil__uK2bxbd",
        "outputId": "2a5c4f58-c7e2-4ba3-ba54-4538628c6929"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply spacy's nlp function to text. \n",
        "# Note: approx. 14mins load time. \n",
        "helpful_df['nlp'] = helpful_df['Text'].apply(nlp)"
      ],
      "metadata": {
        "id": "7VvPmB_5b1dx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe of vectorised text.\n",
        "vectors = helpful_df['nlp'].apply(lambda x: x.vector)\n",
        "vec_df = vectors.apply(pd.Series)\n",
        "vec_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "y8Fs7yByb4th",
        "outputId": "98df51dd-40ed-42b4-8438-a315e0c1bed9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          0         1         2    ...       297       298       299\n",
              "32  -1.323802  1.032652 -2.337208  ... -0.455574 -2.148609  0.903731\n",
              "33  -1.340788  1.367154 -2.432186  ...  0.044586 -2.574825  1.159596\n",
              "82  -1.105859  0.835509 -2.696097  ...  0.323921 -3.517361  1.079162\n",
              "158 -1.373460  0.747225 -1.265600  ... -0.496323 -2.476612  0.582859\n",
              "213 -1.334758  1.621257 -3.277381  ... -0.734187 -3.506289  1.445702\n",
              "\n",
              "[5 rows x 300 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-748d7d6e-2578-4515-84d2-99c553c0fc0f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>-1.323802</td>\n",
              "      <td>1.032652</td>\n",
              "      <td>-2.337208</td>\n",
              "      <td>-0.111059</td>\n",
              "      <td>3.766830</td>\n",
              "      <td>-0.041592</td>\n",
              "      <td>0.173712</td>\n",
              "      <td>3.878510</td>\n",
              "      <td>-0.715135</td>\n",
              "      <td>-0.504095</td>\n",
              "      <td>6.821686</td>\n",
              "      <td>0.671419</td>\n",
              "      <td>-3.210259</td>\n",
              "      <td>1.448479</td>\n",
              "      <td>0.946040</td>\n",
              "      <td>1.138279</td>\n",
              "      <td>1.090314</td>\n",
              "      <td>-0.362776</td>\n",
              "      <td>-0.585832</td>\n",
              "      <td>-2.392319</td>\n",
              "      <td>1.227578</td>\n",
              "      <td>-0.547442</td>\n",
              "      <td>-1.700264</td>\n",
              "      <td>-1.040152</td>\n",
              "      <td>-1.243329</td>\n",
              "      <td>-1.521293</td>\n",
              "      <td>-2.187235</td>\n",
              "      <td>-1.004288</td>\n",
              "      <td>-0.797042</td>\n",
              "      <td>1.756473</td>\n",
              "      <td>0.547730</td>\n",
              "      <td>-0.973817</td>\n",
              "      <td>-1.358144</td>\n",
              "      <td>-1.650358</td>\n",
              "      <td>-1.899574</td>\n",
              "      <td>0.209466</td>\n",
              "      <td>-0.451592</td>\n",
              "      <td>1.834457</td>\n",
              "      <td>1.899464</td>\n",
              "      <td>1.260023</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.259828</td>\n",
              "      <td>4.635929</td>\n",
              "      <td>2.206722</td>\n",
              "      <td>2.675498</td>\n",
              "      <td>1.182052</td>\n",
              "      <td>0.379287</td>\n",
              "      <td>0.680859</td>\n",
              "      <td>1.654571</td>\n",
              "      <td>-4.040608</td>\n",
              "      <td>0.775131</td>\n",
              "      <td>2.621340</td>\n",
              "      <td>-1.087492</td>\n",
              "      <td>0.145141</td>\n",
              "      <td>-0.866788</td>\n",
              "      <td>0.327439</td>\n",
              "      <td>-1.280318</td>\n",
              "      <td>1.733215</td>\n",
              "      <td>-0.229024</td>\n",
              "      <td>-0.496044</td>\n",
              "      <td>2.095020</td>\n",
              "      <td>0.185248</td>\n",
              "      <td>-0.788270</td>\n",
              "      <td>0.125567</td>\n",
              "      <td>0.858253</td>\n",
              "      <td>3.029055</td>\n",
              "      <td>-0.892422</td>\n",
              "      <td>0.234885</td>\n",
              "      <td>1.083938</td>\n",
              "      <td>-1.670763</td>\n",
              "      <td>1.232402</td>\n",
              "      <td>0.677222</td>\n",
              "      <td>-1.280813</td>\n",
              "      <td>1.062335</td>\n",
              "      <td>-0.518964</td>\n",
              "      <td>-1.577349</td>\n",
              "      <td>0.981302</td>\n",
              "      <td>0.494414</td>\n",
              "      <td>-0.455574</td>\n",
              "      <td>-2.148609</td>\n",
              "      <td>0.903731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>-1.340788</td>\n",
              "      <td>1.367154</td>\n",
              "      <td>-2.432186</td>\n",
              "      <td>0.159615</td>\n",
              "      <td>3.363936</td>\n",
              "      <td>-0.144899</td>\n",
              "      <td>0.814567</td>\n",
              "      <td>3.691298</td>\n",
              "      <td>-1.419114</td>\n",
              "      <td>-0.034321</td>\n",
              "      <td>6.347642</td>\n",
              "      <td>1.152521</td>\n",
              "      <td>-3.645756</td>\n",
              "      <td>1.715338</td>\n",
              "      <td>1.291463</td>\n",
              "      <td>0.043991</td>\n",
              "      <td>1.414632</td>\n",
              "      <td>-1.040479</td>\n",
              "      <td>-0.216113</td>\n",
              "      <td>-2.626247</td>\n",
              "      <td>0.523843</td>\n",
              "      <td>-0.010043</td>\n",
              "      <td>-0.843776</td>\n",
              "      <td>-1.263611</td>\n",
              "      <td>-0.619142</td>\n",
              "      <td>-1.571180</td>\n",
              "      <td>-2.437595</td>\n",
              "      <td>-0.664569</td>\n",
              "      <td>-1.327567</td>\n",
              "      <td>1.994807</td>\n",
              "      <td>1.611927</td>\n",
              "      <td>-1.193155</td>\n",
              "      <td>-1.064011</td>\n",
              "      <td>-2.166667</td>\n",
              "      <td>-1.334005</td>\n",
              "      <td>0.065550</td>\n",
              "      <td>0.055779</td>\n",
              "      <td>1.521659</td>\n",
              "      <td>2.446720</td>\n",
              "      <td>2.024306</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.695887</td>\n",
              "      <td>4.200289</td>\n",
              "      <td>2.334938</td>\n",
              "      <td>2.665738</td>\n",
              "      <td>1.054603</td>\n",
              "      <td>0.317752</td>\n",
              "      <td>0.345939</td>\n",
              "      <td>1.617559</td>\n",
              "      <td>-4.228474</td>\n",
              "      <td>0.635725</td>\n",
              "      <td>2.033291</td>\n",
              "      <td>-0.785462</td>\n",
              "      <td>-0.217867</td>\n",
              "      <td>-0.837831</td>\n",
              "      <td>-0.227115</td>\n",
              "      <td>-1.581848</td>\n",
              "      <td>1.882767</td>\n",
              "      <td>-0.708194</td>\n",
              "      <td>-0.849401</td>\n",
              "      <td>1.734663</td>\n",
              "      <td>0.213024</td>\n",
              "      <td>-0.843041</td>\n",
              "      <td>0.213760</td>\n",
              "      <td>0.667129</td>\n",
              "      <td>2.813268</td>\n",
              "      <td>-1.060397</td>\n",
              "      <td>0.598082</td>\n",
              "      <td>1.192488</td>\n",
              "      <td>-1.699868</td>\n",
              "      <td>0.689113</td>\n",
              "      <td>0.708320</td>\n",
              "      <td>-0.879604</td>\n",
              "      <td>1.487295</td>\n",
              "      <td>-0.799887</td>\n",
              "      <td>-1.704227</td>\n",
              "      <td>0.511260</td>\n",
              "      <td>0.882618</td>\n",
              "      <td>0.044586</td>\n",
              "      <td>-2.574825</td>\n",
              "      <td>1.159596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>-1.105859</td>\n",
              "      <td>0.835509</td>\n",
              "      <td>-2.696097</td>\n",
              "      <td>-0.247309</td>\n",
              "      <td>1.856147</td>\n",
              "      <td>-0.053594</td>\n",
              "      <td>0.384191</td>\n",
              "      <td>4.065519</td>\n",
              "      <td>-2.479806</td>\n",
              "      <td>0.900214</td>\n",
              "      <td>5.744823</td>\n",
              "      <td>1.591808</td>\n",
              "      <td>-3.536534</td>\n",
              "      <td>1.848100</td>\n",
              "      <td>1.593867</td>\n",
              "      <td>-0.284174</td>\n",
              "      <td>1.272760</td>\n",
              "      <td>-1.875831</td>\n",
              "      <td>-1.291954</td>\n",
              "      <td>-2.677112</td>\n",
              "      <td>1.189191</td>\n",
              "      <td>0.456339</td>\n",
              "      <td>-0.926521</td>\n",
              "      <td>-1.687548</td>\n",
              "      <td>-1.063437</td>\n",
              "      <td>-1.599960</td>\n",
              "      <td>-2.343125</td>\n",
              "      <td>-0.479117</td>\n",
              "      <td>-1.167389</td>\n",
              "      <td>1.452251</td>\n",
              "      <td>0.794457</td>\n",
              "      <td>-1.676026</td>\n",
              "      <td>-0.853468</td>\n",
              "      <td>-1.515631</td>\n",
              "      <td>-0.690861</td>\n",
              "      <td>-0.089525</td>\n",
              "      <td>0.008364</td>\n",
              "      <td>1.239329</td>\n",
              "      <td>2.485189</td>\n",
              "      <td>1.491956</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026996</td>\n",
              "      <td>4.711818</td>\n",
              "      <td>1.827735</td>\n",
              "      <td>2.757103</td>\n",
              "      <td>2.091994</td>\n",
              "      <td>-0.345337</td>\n",
              "      <td>0.479066</td>\n",
              "      <td>1.463259</td>\n",
              "      <td>-3.679106</td>\n",
              "      <td>0.703237</td>\n",
              "      <td>1.621652</td>\n",
              "      <td>-1.023482</td>\n",
              "      <td>-0.663748</td>\n",
              "      <td>-0.376884</td>\n",
              "      <td>-0.813136</td>\n",
              "      <td>-1.392176</td>\n",
              "      <td>1.399031</td>\n",
              "      <td>-1.134432</td>\n",
              "      <td>-0.523177</td>\n",
              "      <td>1.698136</td>\n",
              "      <td>-0.570428</td>\n",
              "      <td>-0.709891</td>\n",
              "      <td>0.495796</td>\n",
              "      <td>0.965876</td>\n",
              "      <td>2.556315</td>\n",
              "      <td>-0.832498</td>\n",
              "      <td>-0.481100</td>\n",
              "      <td>1.396338</td>\n",
              "      <td>-2.113331</td>\n",
              "      <td>0.968965</td>\n",
              "      <td>0.581942</td>\n",
              "      <td>-0.722913</td>\n",
              "      <td>0.525695</td>\n",
              "      <td>-1.452901</td>\n",
              "      <td>-2.035355</td>\n",
              "      <td>0.041754</td>\n",
              "      <td>0.817126</td>\n",
              "      <td>0.323921</td>\n",
              "      <td>-3.517361</td>\n",
              "      <td>1.079162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>-1.373460</td>\n",
              "      <td>0.747225</td>\n",
              "      <td>-1.265600</td>\n",
              "      <td>-0.159294</td>\n",
              "      <td>2.669593</td>\n",
              "      <td>0.045321</td>\n",
              "      <td>1.209111</td>\n",
              "      <td>3.169600</td>\n",
              "      <td>-2.023155</td>\n",
              "      <td>-0.155882</td>\n",
              "      <td>4.968339</td>\n",
              "      <td>1.249633</td>\n",
              "      <td>-3.969251</td>\n",
              "      <td>0.773692</td>\n",
              "      <td>1.264198</td>\n",
              "      <td>0.472989</td>\n",
              "      <td>1.794931</td>\n",
              "      <td>-0.432815</td>\n",
              "      <td>-0.884058</td>\n",
              "      <td>-0.434370</td>\n",
              "      <td>0.661992</td>\n",
              "      <td>0.199351</td>\n",
              "      <td>-1.122678</td>\n",
              "      <td>0.300546</td>\n",
              "      <td>-1.905074</td>\n",
              "      <td>-1.416467</td>\n",
              "      <td>-1.696853</td>\n",
              "      <td>-1.086357</td>\n",
              "      <td>-0.755377</td>\n",
              "      <td>1.632948</td>\n",
              "      <td>0.737310</td>\n",
              "      <td>-0.848773</td>\n",
              "      <td>-0.652749</td>\n",
              "      <td>-0.921709</td>\n",
              "      <td>-0.135150</td>\n",
              "      <td>-0.834802</td>\n",
              "      <td>-1.014495</td>\n",
              "      <td>0.850796</td>\n",
              "      <td>2.733212</td>\n",
              "      <td>0.976385</td>\n",
              "      <td>...</td>\n",
              "      <td>0.243693</td>\n",
              "      <td>3.581789</td>\n",
              "      <td>2.834831</td>\n",
              "      <td>2.117241</td>\n",
              "      <td>1.317349</td>\n",
              "      <td>-0.134647</td>\n",
              "      <td>-0.204952</td>\n",
              "      <td>1.466174</td>\n",
              "      <td>-3.950817</td>\n",
              "      <td>0.065238</td>\n",
              "      <td>1.224981</td>\n",
              "      <td>-0.850976</td>\n",
              "      <td>-0.036110</td>\n",
              "      <td>-1.002332</td>\n",
              "      <td>0.033793</td>\n",
              "      <td>-0.823109</td>\n",
              "      <td>0.986090</td>\n",
              "      <td>-0.675296</td>\n",
              "      <td>-0.523905</td>\n",
              "      <td>1.640264</td>\n",
              "      <td>0.573491</td>\n",
              "      <td>-1.289775</td>\n",
              "      <td>-0.342641</td>\n",
              "      <td>0.808270</td>\n",
              "      <td>2.397375</td>\n",
              "      <td>-0.467226</td>\n",
              "      <td>1.012726</td>\n",
              "      <td>0.701020</td>\n",
              "      <td>-1.292014</td>\n",
              "      <td>0.111450</td>\n",
              "      <td>1.271905</td>\n",
              "      <td>0.149229</td>\n",
              "      <td>1.511957</td>\n",
              "      <td>-0.618358</td>\n",
              "      <td>-1.302749</td>\n",
              "      <td>1.035043</td>\n",
              "      <td>1.441601</td>\n",
              "      <td>-0.496323</td>\n",
              "      <td>-2.476612</td>\n",
              "      <td>0.582859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>-1.334758</td>\n",
              "      <td>1.621257</td>\n",
              "      <td>-3.277381</td>\n",
              "      <td>-0.617078</td>\n",
              "      <td>3.131202</td>\n",
              "      <td>0.339972</td>\n",
              "      <td>-0.001275</td>\n",
              "      <td>3.923903</td>\n",
              "      <td>-1.103019</td>\n",
              "      <td>0.307257</td>\n",
              "      <td>6.141570</td>\n",
              "      <td>2.204481</td>\n",
              "      <td>-3.501877</td>\n",
              "      <td>1.749801</td>\n",
              "      <td>1.212692</td>\n",
              "      <td>-0.234007</td>\n",
              "      <td>1.003945</td>\n",
              "      <td>-2.364910</td>\n",
              "      <td>-1.858483</td>\n",
              "      <td>-3.014802</td>\n",
              "      <td>0.971603</td>\n",
              "      <td>-0.789706</td>\n",
              "      <td>-0.921547</td>\n",
              "      <td>-1.457055</td>\n",
              "      <td>-0.255474</td>\n",
              "      <td>-1.483295</td>\n",
              "      <td>-3.027544</td>\n",
              "      <td>-0.148319</td>\n",
              "      <td>-1.413745</td>\n",
              "      <td>1.737659</td>\n",
              "      <td>1.762387</td>\n",
              "      <td>-1.727458</td>\n",
              "      <td>-1.128665</td>\n",
              "      <td>-2.265805</td>\n",
              "      <td>-1.974384</td>\n",
              "      <td>-1.223073</td>\n",
              "      <td>-0.306580</td>\n",
              "      <td>1.176712</td>\n",
              "      <td>2.239058</td>\n",
              "      <td>1.466933</td>\n",
              "      <td>...</td>\n",
              "      <td>0.318554</td>\n",
              "      <td>5.029393</td>\n",
              "      <td>2.958174</td>\n",
              "      <td>2.073317</td>\n",
              "      <td>2.297361</td>\n",
              "      <td>-0.700526</td>\n",
              "      <td>0.568019</td>\n",
              "      <td>2.481175</td>\n",
              "      <td>-3.088524</td>\n",
              "      <td>1.123126</td>\n",
              "      <td>1.174182</td>\n",
              "      <td>-0.702311</td>\n",
              "      <td>-0.502268</td>\n",
              "      <td>-1.932580</td>\n",
              "      <td>0.242989</td>\n",
              "      <td>-1.240995</td>\n",
              "      <td>1.314936</td>\n",
              "      <td>-1.698866</td>\n",
              "      <td>-1.386699</td>\n",
              "      <td>1.899207</td>\n",
              "      <td>-0.496450</td>\n",
              "      <td>0.077758</td>\n",
              "      <td>0.821671</td>\n",
              "      <td>1.303991</td>\n",
              "      <td>2.718992</td>\n",
              "      <td>-1.078073</td>\n",
              "      <td>-0.244497</td>\n",
              "      <td>1.780511</td>\n",
              "      <td>-2.109499</td>\n",
              "      <td>0.187792</td>\n",
              "      <td>0.962377</td>\n",
              "      <td>-0.649345</td>\n",
              "      <td>0.305418</td>\n",
              "      <td>-0.671857</td>\n",
              "      <td>-1.519091</td>\n",
              "      <td>0.111250</td>\n",
              "      <td>0.235039</td>\n",
              "      <td>-0.734187</td>\n",
              "      <td>-3.506289</td>\n",
              "      <td>1.445702</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 300 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-748d7d6e-2578-4515-84d2-99c553c0fc0f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-748d7d6e-2578-4515-84d2-99c553c0fc0f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-748d7d6e-2578-4515-84d2-99c553c0fc0f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up TPU for training\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "aER2iaB_b7tn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
        "print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWotxfuvb_N9",
        "outputId": "ce9368b5-a010-41a8-81fc-61cb96cdd682"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.94.44.202:8470']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "pifh0Eb7cAyf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(vec_df, helpful_df['helpful_score'], test_size=0.2, random_state=99)"
      ],
      "metadata": {
        "id": "yXbczK-hcFgb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import add, Bidirectional, Dense, Dropout, GRU\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "TPNR6RUhcHaJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BiGRU model\n",
        "\n",
        "# Parameters\n",
        "optimizer_bigru = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "early_stop_bigru = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True)\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'AUC']\n",
        "\n",
        "# Architecture\n",
        "with tpu_strategy.scope():\n",
        "  layers = [Bidirectional(GRU(128, input_shape=input_shape, activation='relu', return_sequences=True), input_shape=input_shape),\n",
        "            Dropout(0.2),\n",
        "            Bidirectional(GRU(128, activation='relu')),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1, activation='sigmoid')]\n",
        "  model_bigru = Sequential(layers)\n",
        "  model_bigru.compile(loss='binary_crossentropy', optimizer=optimizer_bigru, metrics=metrics)\n",
        "model_bigru.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNxq4BBWcJ_C",
        "outputId": "4808f0ba-5b75-4b42-9121-158ba5748389"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_8 (Bidirectio  (None, 300, 256)         100608    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 300, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 256)              296448    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 415,617\n",
            "Trainable params: 415,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model \n",
        "# Runtime approx. 23mins with Google Colab TPU. Do not run on CPU. \n",
        "history_bigru = model_bigru.fit(X_train, y_train, \n",
        "                                validation_data=(X_test, y_test), \n",
        "                                epochs=100, batch_size=64, \n",
        "                                callbacks=[early_stop_bigru])\n",
        "\n",
        "# Save model\n",
        "model_bigru.save('/content/gdrive/MyDrive/models/anlp_bi_class.h5')\n",
        "\n",
        "# Save history\n",
        "np.save('/content/gdrive/MyDrive/models/anlp_bi_class.npy', history_bigru)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbJ-VDEtckQY",
        "outputId": "0d85f790-01a9-4bec-b9c6-b2778c82290d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 63s 154ms/step - loss: 0.6425 - Accuracy: 0.6568 - precision: 0.6590 - recall: 0.9855 - auc: 0.5508 - val_loss: 0.6277 - val_Accuracy: 0.6602 - val_precision: 0.6638 - val_recall: 0.9686 - val_auc: 0.6238\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.6281 - Accuracy: 0.6659 - precision: 0.6738 - recall: 0.9490 - auc: 0.6026 - val_loss: 0.6246 - val_Accuracy: 0.6628 - val_precision: 0.6655 - val_recall: 0.9690 - val_auc: 0.6319\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6201 - Accuracy: 0.6703 - precision: 0.6767 - recall: 0.9501 - auc: 0.6280 - val_loss: 0.6103 - val_Accuracy: 0.6728 - val_precision: 0.6751 - val_recall: 0.9588 - val_auc: 0.6692\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.6087 - Accuracy: 0.6797 - precision: 0.6904 - recall: 0.9255 - auc: 0.6581 - val_loss: 0.6040 - val_Accuracy: 0.6788 - val_precision: 0.6848 - val_recall: 0.9388 - val_auc: 0.6860\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.5977 - Accuracy: 0.6862 - precision: 0.7040 - recall: 0.8983 - auc: 0.6813 - val_loss: 0.5964 - val_Accuracy: 0.6906 - val_precision: 0.7037 - val_recall: 0.9062 - val_auc: 0.6960\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5909 - Accuracy: 0.6896 - precision: 0.7139 - recall: 0.8774 - auc: 0.6945 - val_loss: 0.5882 - val_Accuracy: 0.6908 - val_precision: 0.7190 - val_recall: 0.8620 - val_auc: 0.7026\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5874 - Accuracy: 0.6966 - precision: 0.7175 - recall: 0.8849 - auc: 0.6997 - val_loss: 0.5865 - val_Accuracy: 0.6954 - val_precision: 0.7137 - val_recall: 0.8887 - val_auc: 0.7103\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.5826 - Accuracy: 0.6973 - precision: 0.7199 - recall: 0.8797 - auc: 0.7086 - val_loss: 0.5850 - val_Accuracy: 0.6928 - val_precision: 0.7317 - val_recall: 0.8340 - val_auc: 0.7114\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.5794 - Accuracy: 0.6972 - precision: 0.7236 - recall: 0.8693 - auc: 0.7134 - val_loss: 0.5801 - val_Accuracy: 0.6968 - val_precision: 0.7116 - val_recall: 0.8982 - val_auc: 0.7176\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 39s 125ms/step - loss: 0.5768 - Accuracy: 0.7000 - precision: 0.7267 - recall: 0.8680 - auc: 0.7173 - val_loss: 0.5800 - val_Accuracy: 0.6938 - val_precision: 0.7220 - val_recall: 0.8614 - val_auc: 0.7164\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.5718 - Accuracy: 0.7052 - precision: 0.7292 - recall: 0.8742 - auc: 0.7243 - val_loss: 0.5856 - val_Accuracy: 0.6896 - val_precision: 0.6959 - val_recall: 0.9293 - val_auc: 0.7194\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.5694 - Accuracy: 0.7039 - precision: 0.7325 - recall: 0.8626 - auc: 0.7280 - val_loss: 0.5749 - val_Accuracy: 0.6974 - val_precision: 0.7194 - val_recall: 0.8773 - val_auc: 0.7250\n",
            "Epoch 13/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5676 - Accuracy: 0.7051 - precision: 0.7332 - recall: 0.8637 - auc: 0.7303 - val_loss: 0.5788 - val_Accuracy: 0.7004 - val_precision: 0.7131 - val_recall: 0.9032 - val_auc: 0.7292\n",
            "Epoch 14/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.5607 - Accuracy: 0.7102 - precision: 0.7392 - recall: 0.8608 - auc: 0.7385 - val_loss: 0.5750 - val_Accuracy: 0.6990 - val_precision: 0.7486 - val_recall: 0.8094 - val_auc: 0.7272\n",
            "Epoch 15/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5576 - Accuracy: 0.7167 - precision: 0.7419 - recall: 0.8697 - auc: 0.7435 - val_loss: 0.5955 - val_Accuracy: 0.6878 - val_precision: 0.7872 - val_recall: 0.7132 - val_auc: 0.7299\n",
            "Epoch 16/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5575 - Accuracy: 0.7131 - precision: 0.7406 - recall: 0.8644 - auc: 0.7438 - val_loss: 0.5631 - val_Accuracy: 0.7058 - val_precision: 0.7272 - val_recall: 0.8770 - val_auc: 0.7417\n",
            "Epoch 17/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.5531 - Accuracy: 0.7179 - precision: 0.7475 - recall: 0.8592 - auc: 0.7491 - val_loss: 0.5571 - val_Accuracy: 0.7130 - val_precision: 0.7508 - val_recall: 0.8371 - val_auc: 0.7475\n",
            "Epoch 18/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5495 - Accuracy: 0.7212 - precision: 0.7504 - recall: 0.8602 - auc: 0.7547 - val_loss: 0.5607 - val_Accuracy: 0.7070 - val_precision: 0.7363 - val_recall: 0.8567 - val_auc: 0.7483\n",
            "Epoch 19/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5440 - Accuracy: 0.7271 - precision: 0.7576 - recall: 0.8573 - auc: 0.7609 - val_loss: 0.5588 - val_Accuracy: 0.7096 - val_precision: 0.7407 - val_recall: 0.8524 - val_auc: 0.7497\n",
            "Epoch 20/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5397 - Accuracy: 0.7315 - precision: 0.7587 - recall: 0.8649 - auc: 0.7653 - val_loss: 0.5459 - val_Accuracy: 0.7200 - val_precision: 0.7491 - val_recall: 0.8571 - val_auc: 0.7625\n",
            "Epoch 21/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.5362 - Accuracy: 0.7356 - precision: 0.7596 - recall: 0.8718 - auc: 0.7691 - val_loss: 0.5562 - val_Accuracy: 0.7180 - val_precision: 0.7804 - val_recall: 0.7888 - val_auc: 0.7591\n",
            "Epoch 22/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5320 - Accuracy: 0.7366 - precision: 0.7637 - recall: 0.8652 - auc: 0.7744 - val_loss: 0.5508 - val_Accuracy: 0.7322 - val_precision: 0.7840 - val_recall: 0.8125 - val_auc: 0.7650\n",
            "Epoch 23/100\n",
            "313/313 [==============================] - 40s 127ms/step - loss: 0.5304 - Accuracy: 0.7401 - precision: 0.7670 - recall: 0.8658 - auc: 0.7766 - val_loss: 0.5444 - val_Accuracy: 0.7238 - val_precision: 0.7548 - val_recall: 0.8528 - val_auc: 0.7656\n",
            "Epoch 24/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.5227 - Accuracy: 0.7463 - precision: 0.7719 - recall: 0.8690 - auc: 0.7847 - val_loss: 0.5467 - val_Accuracy: 0.7206 - val_precision: 0.7422 - val_recall: 0.8746 - val_auc: 0.7623\n",
            "Epoch 25/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5211 - Accuracy: 0.7471 - precision: 0.7704 - recall: 0.8740 - auc: 0.7860 - val_loss: 0.5344 - val_Accuracy: 0.7334 - val_precision: 0.7621 - val_recall: 0.8586 - val_auc: 0.7752\n",
            "Epoch 26/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5158 - Accuracy: 0.7491 - precision: 0.7727 - recall: 0.8735 - auc: 0.7907 - val_loss: 0.5317 - val_Accuracy: 0.7320 - val_precision: 0.7505 - val_recall: 0.8813 - val_auc: 0.7775\n",
            "Epoch 27/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.5203 - Accuracy: 0.7490 - precision: 0.7713 - recall: 0.8763 - auc: 0.7884 - val_loss: 0.5749 - val_Accuracy: 0.6978 - val_precision: 0.7024 - val_recall: 0.9296 - val_auc: 0.7346\n",
            "Epoch 28/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.5156 - Accuracy: 0.7489 - precision: 0.7714 - recall: 0.8758 - auc: 0.7908 - val_loss: 0.5275 - val_Accuracy: 0.7342 - val_precision: 0.7506 - val_recall: 0.8863 - val_auc: 0.7833\n",
            "Epoch 29/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5033 - Accuracy: 0.7622 - precision: 0.7831 - recall: 0.8805 - auc: 0.8027 - val_loss: 0.5234 - val_Accuracy: 0.7390 - val_precision: 0.7619 - val_recall: 0.8715 - val_auc: 0.7872\n",
            "Epoch 30/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.4984 - Accuracy: 0.7633 - precision: 0.7836 - recall: 0.8819 - auc: 0.8069 - val_loss: 0.5195 - val_Accuracy: 0.7456 - val_precision: 0.7653 - val_recall: 0.8789 - val_auc: 0.7913\n",
            "Epoch 31/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.4962 - Accuracy: 0.7620 - precision: 0.7826 - recall: 0.8810 - auc: 0.8091 - val_loss: 0.5285 - val_Accuracy: 0.7408 - val_precision: 0.7781 - val_recall: 0.8420 - val_auc: 0.7875\n",
            "Epoch 32/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.4882 - Accuracy: 0.7688 - precision: 0.7892 - recall: 0.8823 - auc: 0.8167 - val_loss: 0.5236 - val_Accuracy: 0.7420 - val_precision: 0.7788 - val_recall: 0.8432 - val_auc: 0.7893\n",
            "Epoch 33/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.4816 - Accuracy: 0.7747 - precision: 0.7945 - recall: 0.8846 - auc: 0.8214 - val_loss: 0.5230 - val_Accuracy: 0.7501 - val_precision: 0.7826 - val_recall: 0.8531 - val_auc: 0.7958\n",
            "Epoch 34/100\n",
            "313/313 [==============================] - 38s 122ms/step - loss: 0.4803 - Accuracy: 0.7750 - precision: 0.7968 - recall: 0.8810 - auc: 0.8221 - val_loss: 0.5226 - val_Accuracy: 0.7408 - val_precision: 0.7489 - val_recall: 0.9056 - val_auc: 0.7912\n",
            "Epoch 35/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.4729 - Accuracy: 0.7791 - precision: 0.7994 - recall: 0.8843 - auc: 0.8287 - val_loss: 0.5231 - val_Accuracy: 0.7517 - val_precision: 0.7771 - val_recall: 0.8672 - val_auc: 0.7975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neg, pos = np.bincount(y_train)\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbdTDFWAZjmG",
        "outputId": "294ff7e5-9184-4449-9d8d-de907ac4704c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "    Total: 19985\n",
            "    Positive: 13079 (65.44% of total)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXG_jmeuYPtX",
        "outputId": "680bf64e-8e18-43d5-a91e-73466d75b499"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight for class 0: 1.45\n",
            "Weight for class 1: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall, AUC"
      ],
      "metadata": {
        "id": "V-_NqUbBcuB_"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create BiGRU model\n",
        "\n",
        "# Parameters\n",
        "optimizer_imbal = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "early_stop_imbal = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True)\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'AUC']\n",
        "\n",
        "# Architecture\n",
        "with tpu_strategy.scope():\n",
        "  layers = [Bidirectional(GRU(128, input_shape=input_shape, activation='relu', return_sequences=True), input_shape=input_shape),\n",
        "            Dropout(0.2),\n",
        "            Bidirectional(GRU(128, activation='relu')),\n",
        "            Dropout(0.2),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1, activation='sigmoid')]\n",
        "  model_imbal = Sequential(layers)\n",
        "  model_imbal.compile(loss='binary_crossentropy', optimizer=optimizer_imbal, metrics=metrics)\n",
        "model_imbal.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrlfUKY6Z9a3",
        "outputId": "865bf8d7-b522-43dc-f189-9c2317c935f9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_6 (Bidirectio  (None, 300, 256)         100608    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 300, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 256)              296448    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 415,617\n",
            "Trainable params: 415,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Model \n",
        "# Run time approx. 10mins\n",
        "history_imbal = model_imbal.fit(X_train, y_train, \n",
        "                                validation_data=(X_test, y_test), \n",
        "                                epochs=100, batch_size=64, \n",
        "                                callbacks=[early_stop_imbal],\n",
        "                                class_weight=class_weight)\n",
        "\n",
        "# Save model\n",
        "model_imbal.save('/content/gdrive/MyDrive/models/anlp_bi_class_imbal.h5')\n",
        "\n",
        "# Save history\n",
        "np.save('/content/gdrive/MyDrive/models/anlp_bi_class_imbal.npy', history_imbal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvyydonKaXtI",
        "outputId": "7862455f-97cf-4d8b-d3f1-e02c7c167ba1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "313/313 [==============================] - 64s 157ms/step - loss: 0.6915 - Accuracy: 0.5685 - precision: 0.6876 - recall: 0.6243 - auc: 0.5620 - val_loss: 0.6956 - val_Accuracy: 0.5643 - val_precision: 0.7185 - val_recall: 0.5438 - val_auc: 0.6056\n",
            "Epoch 2/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.6780 - Accuracy: 0.6148 - precision: 0.7025 - recall: 0.7137 - auc: 0.5929 - val_loss: 0.6652 - val_Accuracy: 0.6404 - val_precision: 0.7153 - val_recall: 0.7436 - val_auc: 0.6272\n",
            "Epoch 3/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.6680 - Accuracy: 0.6175 - precision: 0.7248 - recall: 0.6697 - auc: 0.6245 - val_loss: 0.6628 - val_Accuracy: 0.6508 - val_precision: 0.7398 - val_recall: 0.7150 - val_auc: 0.6702\n",
            "Epoch 4/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6526 - Accuracy: 0.6386 - precision: 0.7485 - recall: 0.6744 - auc: 0.6619 - val_loss: 0.6792 - val_Accuracy: 0.5737 - val_precision: 0.7849 - val_recall: 0.4756 - val_auc: 0.6861\n",
            "Epoch 5/100\n",
            "313/313 [==============================] - 39s 123ms/step - loss: 0.6424 - Accuracy: 0.6435 - precision: 0.7644 - recall: 0.6582 - auc: 0.6807 - val_loss: 0.6575 - val_Accuracy: 0.6200 - val_precision: 0.7823 - val_recall: 0.5767 - val_auc: 0.6916\n",
            "Epoch 6/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6358 - Accuracy: 0.6483 - precision: 0.7747 - recall: 0.6523 - auc: 0.6930 - val_loss: 0.5999 - val_Accuracy: 0.6906 - val_precision: 0.7273 - val_recall: 0.8395 - val_auc: 0.6994\n",
            "Epoch 7/100\n",
            "313/313 [==============================] - 39s 125ms/step - loss: 0.6310 - Accuracy: 0.6521 - precision: 0.7773 - recall: 0.6566 - auc: 0.6995 - val_loss: 0.5841 - val_Accuracy: 0.6940 - val_precision: 0.7191 - val_recall: 0.8697 - val_auc: 0.7134\n",
            "Epoch 8/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6228 - Accuracy: 0.6595 - precision: 0.7837 - recall: 0.6625 - auc: 0.7130 - val_loss: 0.6032 - val_Accuracy: 0.6798 - val_precision: 0.7683 - val_recall: 0.7276 - val_auc: 0.7223\n",
            "Epoch 9/100\n",
            "313/313 [==============================] - 38s 123ms/step - loss: 0.6153 - Accuracy: 0.6645 - precision: 0.7889 - recall: 0.6656 - auc: 0.7215 - val_loss: 0.6305 - val_Accuracy: 0.6538 - val_precision: 0.8104 - val_recall: 0.6111 - val_auc: 0.7239\n",
            "Epoch 10/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6087 - Accuracy: 0.6686 - precision: 0.8004 - recall: 0.6575 - auc: 0.7309 - val_loss: 0.6345 - val_Accuracy: 0.6504 - val_precision: 0.8240 - val_recall: 0.5887 - val_auc: 0.7315\n",
            "Epoch 11/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.6044 - Accuracy: 0.6732 - precision: 0.8045 - recall: 0.6612 - auc: 0.7359 - val_loss: 0.6187 - val_Accuracy: 0.6608 - val_precision: 0.8180 - val_recall: 0.6160 - val_auc: 0.7366\n",
            "Epoch 12/100\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.5987 - Accuracy: 0.6778 - precision: 0.8046 - recall: 0.6704 - auc: 0.7427 - val_loss: 0.5915 - val_Accuracy: 0.6772 - val_precision: 0.7906 - val_recall: 0.6858 - val_auc: 0.7337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xY-T3V5EbDSl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}